{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e74d5577-df2d-4d38-b98c-6860cb3bab8c",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d625a45-2f9c-4d8f-97b0-2bec958d2d5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T10:50:33.443001Z",
     "iopub.status.busy": "2025-07-08T10:50:33.442831Z",
     "iopub.status.idle": "2025-07-08T10:50:38.234022Z",
     "shell.execute_reply": "2025-07-08T10:50:38.233564Z",
     "shell.execute_reply.started": "2025-07-08T10:50:33.442987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torchaudio\n",
    "from jiwer import wer\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033af508-f8b1-4a7e-a7e4-0c8c68a3da43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T10:50:39.417827Z",
     "iopub.status.busy": "2025-07-08T10:50:39.417291Z",
     "iopub.status.idle": "2025-07-08T10:50:40.735283Z",
     "shell.execute_reply": "2025-07-08T10:50:40.734588Z",
     "shell.execute_reply.started": "2025-07-08T10:50:39.417800Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"hsekhalilian/commonvoice\", split=\"dev\")\n",
    "dataset = dataset.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f10146dd-44b2-4adb-8ad3-6ce381ce86af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T10:56:26.439521Z",
     "iopub.status.busy": "2025-07-08T10:56:26.439235Z",
     "iopub.status.idle": "2025-07-08T10:56:40.529741Z",
     "shell.execute_reply": "2025-07-08T10:56:40.529393Z",
     "shell.execute_reply.started": "2025-07-08T10:56:26.439499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hsekhalilian/wav2vec2-custom-model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2b42d4eab54a379fb62076f51ccd29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae64b4e76634e6da654bf3a0ff023d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325d1fc04bc84fd5ba42b4d1064c4dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136a8c5668824bafb4e63a47bc701bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba74d7798cc94bc688ebe67b9f91f7b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35d3bab8bd347bdb9f9004883950190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"/home/jovyan/.cache/models/m3hrdadfi/wav2vec2-large-xlsr-persian-v3/\"\n",
    "\n",
    "parts = model_path.strip(\"/\").split(\"/\")\n",
    "model_name = f\"{parts[-2]}/{parts[-1]}\"\n",
    "print(model_name)\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_path)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_path).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd0bb76-4843-4e96-b320-aa7433d9609a",
   "metadata": {},
   "source": [
    "# one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "135fc182-26e9-451f-a6e3-b7436f1f76be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T10:58:33.925074Z",
     "iopub.status.busy": "2025-07-08T10:58:33.923463Z",
     "iopub.status.idle": "2025-07-08T10:58:33.981284Z",
     "shell.execute_reply": "2025-07-08T10:58:33.980655Z",
     "shell.execute_reply.started": "2025-07-08T10:58:33.925014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: این اولین قدم برای تغییر خودم\n",
      "Reference: این اولین قدم برای تغییر خودم\n"
     ]
    }
   ],
   "source": [
    "# Load one sample\n",
    "sample = dataset[0]\n",
    "audio = sample[\"audio\"]\n",
    "speech_array, sampling_rate = audio[\"array\"], audio[\"sampling_rate\"]\n",
    "\n",
    "# Resample to 16kHz if needed\n",
    "if sampling_rate != 16000:\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n",
    "    speech_array = resampler(torch.tensor(speech_array)).numpy()\n",
    "\n",
    "# Tokenize and move inputs to GPU\n",
    "inputs = processor(speech_array, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# Decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "# Print results\n",
    "print(\"Prediction:\", transcription)\n",
    "print(\"Reference:\", sample[\"sentence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a6bdb1-be8f-4699-a36c-e890f0d0cc1e",
   "metadata": {},
   "source": [
    "# for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3d5d970-4bcd-41ac-bd12-0e695a6bb598",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T10:58:36.451000Z",
     "iopub.status.busy": "2025-07-08T10:58:36.450677Z",
     "iopub.status.idle": "2025-07-08T10:59:01.739127Z",
     "shell.execute_reply": "2025-07-08T10:59:01.738228Z",
     "shell.execute_reply.started": "2025-07-08T10:58:36.450976Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:25<00:00, 39.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WER: 25.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for sample in tqdm(dataset):\n",
    "    audio = sample[\"audio\"]\n",
    "    speech_array, sampling_rate = audio[\"array\"], audio[\"sampling_rate\"]\n",
    "\n",
    "    # Resample if needed\n",
    "    if sampling_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n",
    "        speech_array = resampler(torch.tensor(speech_array)).numpy()\n",
    "\n",
    "    # Tokenize and move to device\n",
    "    inputs = processor(speech_array, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Decode\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "    # Store results\n",
    "    predictions.append(transcription.lower())\n",
    "    references.append(sample[\"sentence\"].lower())\n",
    "\n",
    "# Compute WER\n",
    "error_rate = wer(references, predictions)\n",
    "print(f\"\\nWER: {error_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33d2b3c-96be-4db1-bb12-05803a971b54",
   "metadata": {},
   "source": [
    "# huggingface datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1490ba54-ccdc-40f8-aa90-10f87dd307f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T12:59:16.749549Z",
     "iopub.status.busy": "2025-07-07T12:59:16.749085Z",
     "iopub.status.idle": "2025-07-07T13:00:08.643942Z",
     "shell.execute_reply": "2025-07-07T13:00:08.642598Z",
     "shell.execute_reply.started": "2025-07-07T12:59:16.749477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fae47e57442433283451b9451743bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/datasets/arrow_dataset.py:3517\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3516\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3517\u001b[39m         \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3518\u001b[39m num_examples_progress_update += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/datasets/arrow_writer.py:552\u001b[39m, in \u001b[36mArrowWriter.write\u001b[39m\u001b[34m(self, example, key, writer_batch_size)\u001b[39m\n\u001b[32m    550\u001b[39m     \u001b[38;5;28mself\u001b[39m.hkey_record = []\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_examples_on_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/datasets/arrow_writer.py:510\u001b[39m, in \u001b[36mArrowWriter.write_examples_on_file\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    506\u001b[39m         batch_examples[col] = [\n\u001b[32m    507\u001b[39m             row[\u001b[32m0\u001b[39m][col].to_pylist()[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row[\u001b[32m0\u001b[39m][col], (pa.Array, pa.ChunkedArray)) \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[32m0\u001b[39m][col]\n\u001b[32m    508\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_examples\n\u001b[32m    509\u001b[39m         ]\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[38;5;28mself\u001b[39m.current_examples = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/datasets/arrow_writer.py:626\u001b[39m, in \u001b[36mArrowWriter.write_batch\u001b[39m\u001b[34m(self, batch_examples, writer_batch_size, try_original_type)\u001b[39m\n\u001b[32m    625\u001b[39m typed_sequence = OptimizedTypedSequence(col_values, \u001b[38;5;28mtype\u001b[39m=col_type, try_type=col_try_type, col=col)\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m arrays.append(\u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyped_sequence\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    627\u001b[39m inferred_features[col] = typed_sequence.get_inferred_type()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pyarrow/array.pxi:255\u001b[39m, in \u001b[36mpyarrow.lib.array\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pyarrow/array.pxi:117\u001b[39m, in \u001b[36mpyarrow.lib._handle_arrow_array_protocol\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/datasets/arrow_writer.py:258\u001b[39m, in \u001b[36mTypedSequence.__arrow_array__\u001b[39m\u001b[34m(self, type)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    255\u001b[39m     \u001b[38;5;66;03m# We use cast_array_to_feature to support casting to custom types like Audio and Image\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;66;03m# Also, when trying type \"string\", we don't want to convert integers or floats to \"string\".\u001b[39;00m\n\u001b[32m    257\u001b[39m     \u001b[38;5;66;03m# We only do it if trying_type is False - since this is what the user asks for.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     out = \u001b[43mcast_array_to_feature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_primitive_to_str\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrying_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_decimal_to_str\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrying_type\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/datasets/table.py:1798\u001b[39m, in \u001b[36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[39m\u001b[34m(array, *args, **kwargs)\u001b[39m\n\u001b[32m   1797\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1798\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/datasets/table.py:1996\u001b[39m, in \u001b[36mcast_array_to_feature\u001b[39m\u001b[34m(array, feature, allow_primitive_to_str, allow_decimal_to_str)\u001b[39m\n\u001b[32m   1995\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(feature, \u001b[33m\"\u001b[39m\u001b[33mcast_storage\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1996\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfeature\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1998\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m pa.types.is_struct(array.type):\n\u001b[32m   1999\u001b[39m     \u001b[38;5;66;03m# feature must be a dict or Sequence(subfeatures_dict)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/datasets/features/audio.py:231\u001b[39m, in \u001b[36mAudio.cast_storage\u001b[39m\u001b[34m(self, storage)\u001b[39m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m pa.types.is_struct(storage.type) \u001b[38;5;129;01mand\u001b[39;00m storage.type.get_all_field_indices(\u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     storage = pa.array([Audio().encode_example(x) \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstorage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pylist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m pa.types.is_struct(storage.type):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pyarrow/array.pxi:1775\u001b[39m, in \u001b[36mpyarrow.lib.Array.to_pylist\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pyarrow/scalar.pxi:1035\u001b[39m, in \u001b[36mpyarrow.lib.StructScalar.as_py\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen _collections_abc>:819\u001b[39m, in \u001b[36mkeys\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sample\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m dataset = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_and_predict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/datasets/arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/datasets/arrow_dataset.py:3079\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3074\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3075\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3076\u001b[39m         total=pbar_total,\n\u001b[32m   3077\u001b[39m         desc=desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3078\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3079\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/datasets/arrow_dataset.py:3552\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[32m   3551\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3552\u001b[39m         \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tmp_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3554\u001b[39m         tmp_file.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/datasets/arrow_writer.py:657\u001b[39m, in \u001b[36mArrowWriter.finalize\u001b[39m\u001b[34m(self, close_stream)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# Re-initializing to empty list for next batch\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28mself\u001b[39m.hkey_record = []\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_examples_on_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;66;03m# If schema is known, infer features even if no examples were written\u001b[39;00m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.schema:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/datasets/arrow_writer.py:510\u001b[39m, in \u001b[36mArrowWriter.write_examples_on_file\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    505\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    506\u001b[39m         batch_examples[col] = [\n\u001b[32m    507\u001b[39m             row[\u001b[32m0\u001b[39m][col].to_pylist()[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row[\u001b[32m0\u001b[39m][col], (pa.Array, pa.ChunkedArray)) \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[32m0\u001b[39m][col]\n\u001b[32m    508\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_examples\n\u001b[32m    509\u001b[39m         ]\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[38;5;28mself\u001b[39m.current_examples = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/datasets/arrow_writer.py:626\u001b[39m, in \u001b[36mArrowWriter.write_batch\u001b[39m\u001b[34m(self, batch_examples, writer_batch_size, try_original_type)\u001b[39m\n\u001b[32m    620\u001b[39m         col_try_type = (\n\u001b[32m    621\u001b[39m             try_features[col]\n\u001b[32m    622\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m try_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m try_features \u001b[38;5;129;01mand\u001b[39;00m try_original_type\n\u001b[32m    623\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    624\u001b[39m         )\n\u001b[32m    625\u001b[39m         typed_sequence = OptimizedTypedSequence(col_values, \u001b[38;5;28mtype\u001b[39m=col_type, try_type=col_try_type, col=col)\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m         arrays.append(\u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyped_sequence\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    627\u001b[39m         inferred_features[col] = typed_sequence.get_inferred_type()\n\u001b[32m    628\u001b[39m schema = inferred_features.arrow_schema \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.schema\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pyarrow/array.pxi:255\u001b[39m, in \u001b[36mpyarrow.lib.array\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pyarrow/array.pxi:117\u001b[39m, in \u001b[36mpyarrow.lib._handle_arrow_array_protocol\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/datasets/arrow_writer.py:258\u001b[39m, in \u001b[36mTypedSequence.__arrow_array__\u001b[39m\u001b[34m(self, type)\u001b[39m\n\u001b[32m    253\u001b[39m     \u001b[38;5;66;03m# otherwise we can finally use the user's type\u001b[39;00m\n\u001b[32m    254\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    255\u001b[39m         \u001b[38;5;66;03m# We use cast_array_to_feature to support casting to custom types like Audio and Image\u001b[39;00m\n\u001b[32m    256\u001b[39m         \u001b[38;5;66;03m# Also, when trying type \"string\", we don't want to convert integers or floats to \"string\".\u001b[39;00m\n\u001b[32m    257\u001b[39m         \u001b[38;5;66;03m# We only do it if trying_type is False - since this is what the user asks for.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m         out = \u001b[43mcast_array_to_feature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m            \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_primitive_to_str\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrying_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_decimal_to_str\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrying_type\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[32m    263\u001b[39m     \u001b[38;5;167;01mTypeError\u001b[39;00m,\n\u001b[32m    264\u001b[39m     pa.lib.ArrowInvalid,\n\u001b[32m    265\u001b[39m     pa.lib.ArrowNotImplementedError,\n\u001b[32m    266\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# handle type errors and overflows\u001b[39;00m\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# Ignore ArrowNotImplementedError caused by trying type, otherwise re-raise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/datasets/table.py:1798\u001b[39m, in \u001b[36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[39m\u001b[34m(array, *args, **kwargs)\u001b[39m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pa.chunked_array([func(chunk, *args, **kwargs) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m array.chunks])\n\u001b[32m   1797\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1798\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/datasets/table.py:1996\u001b[39m, in \u001b[36mcast_array_to_feature\u001b[39m\u001b[34m(array, feature, allow_primitive_to_str, allow_decimal_to_str)\u001b[39m\n\u001b[32m   1994\u001b[39m     array = array.storage\n\u001b[32m   1995\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(feature, \u001b[33m\"\u001b[39m\u001b[33mcast_storage\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1996\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfeature\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1998\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m pa.types.is_struct(array.type):\n\u001b[32m   1999\u001b[39m     \u001b[38;5;66;03m# feature must be a dict or Sequence(subfeatures_dict)\u001b[39;00m\n\u001b[32m   2000\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature, Sequence) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature.feature, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/datasets/features/audio.py:231\u001b[39m, in \u001b[36mAudio.cast_storage\u001b[39m\u001b[34m(self, storage)\u001b[39m\n\u001b[32m    229\u001b[39m     storage = pa.StructArray.from_arrays([storage, path_array], [\u001b[33m\"\u001b[39m\u001b[33mbytes\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m], mask=storage.is_null())\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m pa.types.is_struct(storage.type) \u001b[38;5;129;01mand\u001b[39;00m storage.type.get_all_field_indices(\u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     storage = pa.array([Audio().encode_example(x) \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstorage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pylist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m pa.types.is_struct(storage.type):\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m storage.type.get_field_index(\u001b[33m\"\u001b[39m\u001b[33mbytes\u001b[39m\u001b[33m\"\u001b[39m) >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pyarrow/array.pxi:1775\u001b[39m, in \u001b[36mpyarrow.lib.Array.to_pylist\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pyarrow/scalar.pxi:1035\u001b[39m, in \u001b[36mpyarrow.lib.StructScalar.as_py\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen _collections_abc>:819\u001b[39m, in \u001b[36mkeys\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def preprocess_and_predict(sample):\n",
    "    try:\n",
    "        speech_array, sampling_rate = sample[\"audio\"][\"array\"], sample[\"audio\"][\"sampling_rate\"]\n",
    "      \n",
    "        if speech_array is None or len(speech_array) == 0:\n",
    "            batch[\"prediction\"] = \"\"\n",
    "            batch[\"reference\"] = batch[\"sentence\"].lower()\n",
    "            return batch\n",
    "        \n",
    "        if sampling_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n",
    "            speech_array = resampler(torch.tensor(speech_array)).numpy()\n",
    "    \n",
    "        inputs = processor(speech_array, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        if device == \"cuda\":\n",
    "            _ = torch.tensor([0.], device=device)  # dummy op\n",
    "            torch.cuda.synchronize()\n",
    "    \n",
    "    \n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.decode(predicted_ids[0])\n",
    "    \n",
    "        sample[\"prediction\"] = transcription\n",
    "        sample[\"reference\"] = sample[\"sentence\"]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with sample: {e}\")\n",
    "        sample[\"prediction\"] = \"\"\n",
    "        sample[\"reference\"] = sample[\"sentence\"]\n",
    "    \n",
    "    return sample\n",
    "\n",
    "# Apply the function\n",
    "dataset = dataset.map(preprocess_and_predict, num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e88aff6-45bb-4795-9914-d0f6c2d64dff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
